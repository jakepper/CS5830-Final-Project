{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5830 Final Project\n",
    "\n",
    "## Ensemble Boosting & Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting Imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df = pd.read_csv('./data/speeddating.csv')\n",
    "display(dating_df.head())\n",
    "print(f'Dataset Shape: {dating_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in dating_df.columns:\n",
    "   print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Null Values per Column:\")\n",
    "null_counts = dating_df.isnull().sum()\n",
    "for col, count in null_counts.items():\n",
    "    if count > 0:\n",
    "       print(f\"{col}: {count}\")\n",
    "\n",
    "print(f\"\\nNumber of Rows with NA values: {dating_df[dating_df.isnull().any(axis=1)].shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column is empty\n",
    "dating_df = dating_df.drop('has_null', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't drop samples with missing values as that would lead to a significant loss of data\n",
    "\n",
    "Let's drop columns where there are over 1000 missing values and drop rows where the majority of the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [col for col, count in null_counts.items() if count > 1000]\n",
    "print(drop)\n",
    "dating_df = dating_df.drop(columns=drop) # drop columns\n",
    "dating_df = dating_df.dropna(subset=['sports']) # drop samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imput the remaining missing values (using median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = dating_df.isnull().sum()\n",
    "\n",
    "for col, count in null_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count}\")\n",
    "        median = dating_df[col].median()\n",
    "        dating_df[col] = dating_df[col].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dating_df.columns:\n",
    "    if dating_df[col].dtype == object:\n",
    "      encoder = OrdinalEncoder()\n",
    "      dating_df[col] = encoder.fit_transform(dating_df[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'match'\n",
    "X = dating_df.drop([target], axis=1)\n",
    "y = dating_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts(normalize=True))\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dating_df.corr()\n",
    "print(\"Correlation with the target variable:\")\n",
    "print(corr['match'].sort_values(ascending=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models\n",
    "\n",
    "Note: LogisticRegression - SVC - Naive Bayes - Decision Tree - KNeighborsClassifier - Neural Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(class_weight={0: class_weights[0], 1: class_weights[1]})\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1_macro')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(scaler.transform(X_test))\n",
    "print(\"Logistic regression performance with class weights:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Bias Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Ensembles\n",
    "\n",
    "Note: AdaBoost - XGBoost - GammaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensembles\n",
    "\n",
    "Note: sklearn.ensemble.BaggingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
