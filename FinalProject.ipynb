{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5830 Final Project\n",
    "\n",
    "## Ensemble Boosting & Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting Imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "# XG/Gamma\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df = pd.read_csv('./data/speeddating.csv')\n",
    "display(dating_df.head())\n",
    "print(f'Dataset Shape: {dating_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in dating_df.columns:\n",
    "   print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df = dating_df.drop(['has_null', 'd_age', 'd_d_age', 'samerace', 'd_importance_same_race', 'd_importance_same_religion', 'd_pref_o_attractive',\n",
    "                            'd_pref_o_sincere', 'd_pref_o_intelligence', 'd_pref_o_funny', 'd_pref_o_ambitious', 'd_pref_o_shared_interests',\n",
    "                            'd_attractive_o', 'd_sinsere_o', 'd_intelligence_o', 'd_funny_o', 'd_ambitous_o', 'd_shared_interests_o',\n",
    "                            'd_attractive_important', 'd_sincere_important', 'd_intellicence_important', 'd_funny_important', 'd_ambtition_important',\n",
    "                            'd_shared_interests_important', 'd_attractive', 'd_sincere', 'd_intelligence', 'd_funny', 'd_ambition', 'd_attractive_partner', \n",
    "                            'd_sincere_partner', 'd_intelligence_partner', 'd_funny_partner', 'd_ambition_partner', 'd_shared_interests_partner',\n",
    "                            'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
    "                            'movies', 'concerts', 'music', 'shopping', 'yoga', 'd_sports', 'd_tvsports', 'd_exercise', 'd_dining', 'd_museums', 'd_art', 'd_hiking', \n",
    "                            'd_gaming', 'd_clubbing', 'd_reading', 'd_tv', 'd_theater', 'd_movies', 'd_concerts', 'd_music', 'd_shopping', 'd_yoga', 'd_interests_correlate', \n",
    "                            'd_expected_happy_with_sd_people', 'd_expected_num_interested_in_me', 'd_expected_num_matches', 'd_like', 'd_guess_prob_liked'\n",
    "                            ], axis=1)\n",
    "display(dating_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Null Values per Column:\")\n",
    "null_counts = dating_df.isnull().sum()\n",
    "for col, count in null_counts.items():\n",
    "    if count > 0:\n",
    "       print(f\"{col}: {count}\")\n",
    "\n",
    "print(f\"\\nNumber of Rows with NA values: {dating_df[dating_df.isnull().any(axis=1)].shape[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't drop samples with missing values as that would lead to a significant loss of data\n",
    "\n",
    "Let's drop columns where there are over 1000 missing values and drop rows where the majority of the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [col for col, count in null_counts.items() if count > 1000]\n",
    "dating_df = dating_df.drop(columns=drop) # drop columns\n",
    "print(f\"\\nNumber of Rows with NA values: {dating_df[dating_df.isnull().any(axis=1)].shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imput the remaining missing values (using median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dating_df.drop(['match', 'decision', 'decision_o'], axis=1, inplace=False)\n",
    "y = dating_df['match']\n",
    "\n",
    "matches = {\"b'0'\": 0, \"b'1'\": 1}\n",
    "\n",
    "y = pd.DataFrame([matches[item] for item in y])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X = pd.DataFrame(imputer.fit_transform(X, y))\n",
    "\n",
    "print(f\"\\nNumber of Rows with NA values: {X[X.isnull().any(axis=1)].shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    if X[col].dtype == object:\n",
    "      encoder = OrdinalEncoder()\n",
    "      X[col] = encoder.fit_transform(X[[col]])\n",
    "        \n",
    "X.columns = dating_df.drop(['match', 'decision', 'decision_o'], axis=1, inplace=False).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts(normalize=True))\n",
    "sns.countplot(data=y, x=y[0])\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "# plt.savefig(\"./figures/target-variable-dist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation with the target variable:\")\n",
    "print(X.corrwith(y[0]).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "y_train = y_train.values.ravel()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "   \"models\": [],\n",
    "   \"scores\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model - Logistic Regression\n",
    "model_lr = LogisticRegression(class_weight='balanced')\n",
    "base_models['models'].append('LogisticRegression')\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(model_lr, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "y_pred = model_lr.predict(scaler.transform(X_test))\n",
    "print(\"Logistic regression performance with class weights:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 score:\", f1)\n",
    "base_models['scores'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# model - Support Vector Machine Classifier\n",
    "model_svc = SVC(class_weight='balanced')\n",
    "base_models['models'].append('SVC')\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(model_svc, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "model_svc.fit(X_train_scaled, y_train)\n",
    "y_pred = model_svc.predict(scaler.transform(X_test))\n",
    "print(\"SVC performance with class weights:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "base_models['scores'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# model - Naive Bayes\n",
    "model_nb = GaussianNB()\n",
    "base_models['models'].append('GaussianNB')\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(model_nb, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "model_nb.fit(X_train_scaled, y_train)\n",
    "y_pred = model_nb.predict(scaler.transform(X_test))\n",
    "print(\"Naive Bayes performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "base_models['scores'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model - K-Nearest Neighbors\n",
    "model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "base_models['models'].append('K-Nearest Neighbors')\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(model_knn, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "model_knn.fit(X_train_scaled, y_train)\n",
    "y_pred = model_knn.predict(scaler.transform(X_test))\n",
    "print(\"K-Nearest Neighbors performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "base_models['scores'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model - Decision Tree Classifier\n",
    "model_dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "base_models['models'].append('Decision Tree')\n",
    "\n",
    "# train / cross-validation\n",
    "cv_scores = cross_val_score(model_dt, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(\"Cross-validation F1-scores:\", cv_scores)\n",
    "print(\"Average F1-score:\", np.mean(cv_scores))\n",
    "print()\n",
    "\n",
    "# test\n",
    "model_dt.fit(X_train_scaled, y_train)\n",
    "y_pred = model_dt.predict(scaler.transform(X_test))\n",
    "print(\"Decision Tree performance with class weights:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 score:\", f1)\n",
    "base_models['scores'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=base_models['models'], y=base_models['scores'])\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.savefig(\"./figures/base-model-scores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Ensembles\n",
    "\n",
    "Note: AdaBoost - XGBoost - GammaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ada = AdaBoostClassifier(model_lr, random_state=123)\n",
    "lr_ada.fit(X_train, y_train)\n",
    "lr_ada.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ada = AdaBoostClassifier(model_lr, random_state=123, n_estimators=2)\n",
    "lr_ada.fit(X_train, y_train)\n",
    "lr_ada.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ada = AdaBoostClassifier(model_dt, random_state=123)\n",
    "dt_ada.fit(X_train, y_train)\n",
    "dt_ada.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5, n_jobs=-1, class_weight='balanced', )\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_log = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=123)\n",
    "xgb_model_log.fit(X_train, y_train)\n",
    "\n",
    "xgb_model_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_hinge = xgb.XGBClassifier(objective=\"binary:hinge\", random_state=123)\n",
    "xgb_model_hinge.fit(X_train, y_train)\n",
    "\n",
    "xgb_model_hinge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_logitraw = xgb.XGBClassifier(objective=\"binary:logitraw\", random_state=123)\n",
    "xgb_model_logitraw.fit(X_train, y_train)\n",
    "\n",
    "xgb_model_logitraw.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensembles\n",
    "\n",
    "Note: sklearn.ensemble.BaggingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
